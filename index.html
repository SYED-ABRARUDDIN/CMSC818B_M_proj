<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title> MAGEC- based MARL Algorithm </title>
    <link rel="stylesheet" href="style.css">
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
   
    <header>
        <h1> MAGEC- based MARL Algorithm</h1>
        <h2>Resilient Distributed Coordination of Multi-Robot Systems</h2>
    </header>
    <section id="introduction">
        <h2>Introduction</h2>

        <p>
        Multi-agent systems are at the forefront of evolving industrial environments and emergency response scenarios, where multiple robots or agents must work cohesively to achieve common goals. </p>
        <p> Here we focus on solving the benchmark problem of the multi-robot patrolling with challenges of agent attrition and partial observability. In the patrolling problem, agents must repeatedly visit a set of observation points or nodes, attempting to minimize one of several metrics such as, the average idleness time or the worst node idleness time.</p>
        <math>\zeta = \frac{1}{m} \sum_{v \in V} \zeta(v) </math>
        <math> \zeta_{max} = \max_{v \in V} \zeta(v) </math>
        
 </section>

    
    <section id="design-methodology">
        <h2>Design Methodology</h2>
 
        <p>The approach used is to employ Multi-Agent Graph Embedding-based Coordination (MAGEC) based on a MARL algorithm designed for resilient distributed coordination of multi-robot systems, particularly in scenarios prone to disturbances like agent attrition and partial observability.
        MAGEC leverages graph neural networks (GNNs) and multi-agent proximal policy optimization (MAPPO) to enable robust coordination. MAGEC uses an actor-critic architecture, with a custom k-convolution GNN serving as the actor. Given graph-based inputs, the actor must select an edge of the graph for the agent to traverse next.</</p>
        

        <p> Role of GNNs:</p>

        <p>Many real-world environments and problems, such as road networks, forests, and warehouses, can be naturally represented as graphs. GNNs, with their ability to learn embeddings of graphs by aggregating information from nodes and edges, are well-suited for modeling and analyzing such environments. They can capture the attributes of individual entities and the complex relationships between them, making them a natural fit for multi-agent coordination tasks.</p>
        
        
        <h3>MAGEC Architecture </h3>

        <p>MAGEC employs an actor-critic architecture, and trained entirely through the MAPPO algorithm. This architecture enables distributed coordination among agents while optimizing for global objectives.</p>

        <ul>
                
                <figure>
                    <img src="images/MAGEC training architecture.png" alt="Graph showing MAGEC performance in communication-limited scenarios" style="width:50%;">
                    <figcaption>Figure 1: Figure shows the MAGEC training architecture. Note that the critic is only used during training (CTDE)</figcaption>
                </figure>
        
        </ul>

        
            <p>Centralized Training, Decentralized Execution (CTDE):The architecture of MAGEC is built on a robust actor-critic framework, where the 'actor' and 'critic' components play distinct yet complementary roles. The actor component is responsible for making decisions based on the policy derived from the current state, whereas the critic evaluates the action taken by the actor by computing the potential reward. This separation enhances the systemâ€™s ability to learn and adapt from each action's consequences, optimizing both the decision-making process and the subsequent outcomes in dynamic environments.
                After training, only the actor network is used for execution, allowing decentralized coordination. </p>
           
        <p> Critic Network: The critic network is a simple multi-layer perceptron (MLP) that takes global information, like node idleness times and an adjacency matrix, to estimate the value of the current state. </p>
           
        <p>Actor Network: The actor network is more complex, utilizing a custom k-convolution GNN. It receives graph-based observations, performs neighbor scoring using an MLP, and finally selects an action (edge to traverse) based on the output of another "selector" MLP. </p>
            
        <p>Discrete Wayfinding:MAGEC enables agents to navigate the graph environment by assigning identifiers to neighboring nodes, enforcing an order for action selection. </p>
            
        <p>Graph Neural Network Design: The Graph Neural Network (GNN) within MAGEC is specifically tailored to support the unique requirements of multi-agent environments. Key adaptations have been made to the traditional GraphSAGE algorithm to better handle edge attributes, which are critical in pathfinding and navigating complex network topologies. These modifications allow the GNN to incorporate both node and edge data effectively, enabling a richer representation of the environment that aids in more accurate and informed decision-making processes
                It utilizes a k-hop message passing mechanism with skip connections for efficient training and embedding of large graphs.  </p>
           
        <p>Neighbor Scoring: To select the best edge to traverse, MAGEC employs a "neighbor scoring" strategy, where graph embeddings of neighboring nodes are processed by an MLP to generate scores, which are then used by the selector MLP to determine the action. </p>
           
        <p>Training Environment: The training environment utilizes graph-based observations, where agents perceive node and edge features within a limited radius. Rewards are given both locally (for visiting nodes) and globally (at the end of each episode) to optimize for minimizing average node idleness time. </p>
           
        <p>Training Algorithm: A modified MAPPO algorithm is used for training, adapted to handle sparse rewards and actions inherent in the patrolling problem. This involves skipping time steps when actions are not required, reducing the number of samples stored and speeding up training. </p>
            
        <p>Execution: After training, the learned policy is directly applied to agents during execution, enabling them to select actions based on their local observations.</p>
            

        <ul>
            
                <figure>
                    <img src="images/GNN computing node embeddings.png" alt="Graph showing MAGEC performance in communication-limited scenarios" style="width:50%;">
                    <figcaption>Figure 2: Figure shows GNN computing node embeddings through iterative neighborhood aggregation. A scoring function is applied to the embeddings (nodes & edges)
 for decision-making.
            </figcaption>
                </figure>
        </ul>
        
</section>
       
    <section id="conclusion">
        <h2>Conclusion</h2>
        <p>MAGEC demonstrates the potential of GNN-based MARL for robust multi-robot coordination in graph environments.  Its resilience to agent attrition and partial observability make it a better approach for real-world applications.
        </p></section>
    
    <section id="references">
        <h2>References</h2>
        <p>The following works provide foundational insights and methodologies that have influenced the development of the MAGEC system:</p>
        <ul>
            <li>Yu, C., Velu, A., Vinitsky, E., Gao, J., Wang, Y., Bayen, A., & Wu, Y. (2022). The Surprising Effectiveness of PPO in Cooperative Multi-Agent Games. </li>
            <li>Goeckner, A., Sui, Y., Martinet, N., Li, X., & Zhu, Q. (2023). Graph Neural Network-based Multi-agent Reinforcement Learning for Resilient Distributed Coordination of Multi-Robot Systems.</li>
            
    </section>
    
    <footer>
        <p>Developed by Abraruddin Syed and Shivam Dhakad.</p>
    </footer>
</body>
</html>
